{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0jr_Q-wl6-0"
   },
   "source": [
    "# Deepchecks RAG LLM Chatbot Workshop\n",
    "\n",
    "- A mini app to create a simple RAG LLM chatbot\n",
    "- Source data could be URLs, PDFs, or HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "R8HGxWTAl6-1",
    "outputId": "e41df59d-1c36-4afe-cef4-47ca154dc273"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\44779\\miniconda3\\envs\\nlp_venv\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\44779\\miniconda3\\envs\\nlp_venv\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain openai chromadb tiktoken beautifulsoup4 pypdf unstructured tqdm langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXkPfee5WKb6"
   },
   "outputs": [],
   "source": [
    "model_name = \"gpt-4o-mini\"  # foundational model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8sEqXP7l6-2"
   },
   "source": [
    "Now, let's import the necessary libraries and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtdxWdDWl6-2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import UnstructuredURLLoader, PyPDFLoader, UnstructuredHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma   \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHvjp6I1WMAp"
   },
   "outputs": [],
   "source": [
    "# Set your OpenAI API key here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnRE0VLsl6-2"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "1. Define documents to upload to vectorstore\n",
    "2. Define System prompt & prompt template\n",
    "3. Define Retrieval system parameters ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGJzF2lll6-2"
   },
   "outputs": [],
   "source": [
    "# System prompt + prompt template\n",
    "system_template = \"\"\"You are an AI assistant that answers questions based on the given context.\n",
    "Your responses should be informative and relevant to the question asked.\n",
    "If you don't know the answer or if the information is not present in the context, say so.\"\"\"\n",
    "\n",
    "human_template = \"\"\"Context: {context}\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "# Retrieval system parameters\n",
    "chunk_size = 1000 # more info to LLM vs\n",
    "chunk_overlap = 200\n",
    "k = 4  # No. relevant chunks . Tradeoff - more info, but might confuse bot, or might add unnecessarily indepth answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlK08De8l6-2"
   },
   "source": [
    "## RAG Chatbot Setup\n",
    "\n",
    "Now, let's set up our RAG chatbot using the configurations we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qy04w7POl6-2"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def setup_rag_chatbot(urls, pdfs, htmls, system_template, human_template, chunk_size, chunk_overlap, k, model_name):\n",
    "    \n",
    "    def load_data(urls, pdfs, htmls):\n",
    "        documents = []\n",
    "        if urls:\n",
    "            url_loader = UnstructuredURLLoader(urls=urls)\n",
    "            documents.extend(url_loader.load())\n",
    "        for pdf in pdfs:\n",
    "            pdf_loader = PyPDFLoader(pdf)\n",
    "            documents.extend(pdf_loader.load())\n",
    "        for html in htmls:\n",
    "            html_loader = UnstructuredHTMLLoader(html)\n",
    "            documents.extend(html_loader.load())\n",
    "        return documents\n",
    "\n",
    "    documents = load_data(urls, pdfs, htmls)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Instantiate pre-trained embedding model & vector store\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(texts, embeddings)\n",
    "    # Retrieve top k results based on similarity score - uses cosine here i.e. dot product between query and vector embedding\n",
    "    retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={\"k\": k})  \n",
    "\n",
    "    # Instantiate LLM # \n",
    "    llm = ChatOpenAI(model_name=model_name, temperature=0)\n",
    "\n",
    "    # Create Prompt # \n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "    # Create RAG chain\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",  # retrieved docs concatenated into single string. Simplest. Best if small enough to fit within LLM's token limits \n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": chat_prompt}\n",
    "    )\n",
    "    print(\"RAG Chatbot is ready!\")\n",
    "    return rag_chain\n",
    "\n",
    "def ask_rag_chatbot(question, rag_chain):\n",
    "    result = rag_chain({\"query\": question})\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": result[\"result\"],\n",
    "        \"sources\": [doc.metadata.get('source', 'Unknown source') for doc in result[\"source_documents\"]]\n",
    "    }\n",
    "\n",
    "def ask_multiple_questions(rag_chain, questions):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(partial(ask_rag_chatbot, rag_chain=rag_chain), questions), total=len(questions)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc7MpW_Al6-3"
   },
   "source": [
    "## Using the RAG Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RT19BVRay_G-",
    "outputId": "15f18c1a-5573-4a5f-d069-1011371c5579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chatbot is ready!\n",
      "Question: What is artificial intelligence?\n",
      "Answer: Artificial intelligence (AI) is the intelligence exhibited by machines, particularly computer systems. It is a field of research within computer science that focuses on developing methods and software that enable machines to perceive their environment and use learning and intelligence to take actions aimed at achieving defined goals. AI encompasses various approaches and applications, including machine learning, natural language processing, robotics, and more.\n",
      "Sources:\n",
      "- https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "- https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "- https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "- https://en.wikipedia.org/wiki/Artificial_intelligence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is machine learning?\n",
      "Answer: Machine learning (ML) is a field of study within artificial intelligence that focuses on the development and analysis of statistical algorithms that enable systems to learn from data and generalize to new, unseen data. This allows machines to perform tasks without explicit instructions. ML encompasses various types of learning, including supervised learning (which involves labeled training data) and unsupervised learning (which identifies patterns in data without guidance). It finds applications in numerous fields such as natural language processing, computer vision, speech recognition, and predictive analytics in business. The foundational methods of ML are rooted in statistics and mathematical optimization.\n",
      "Sources:\n",
      "- https://en.wikipedia.org/wiki/Machine_learning\n",
      "- https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "- https://en.wikipedia.org/wiki/Machine_learning\n",
      "- https://en.wikipedia.org/wiki/Machine_learning\n",
      "\n",
      "Question: How does deep learning relate to AI?\n",
      "Answer: Deep learning is a subset of machine learning, which itself is a key component of artificial intelligence (AI). The relationship between deep learning and AI can be traced back to the early days of AI research, where some researchers sought to enable machines to learn from data using various methods, including symbolic approaches and early neural networks. \n",
      "\n",
      "Deep learning began to dominate industry benchmarks around 2012, leading to its widespread adoption across the field of AI. Its success is attributed to significant advancements in computer hardware, such as faster computers and the use of graphics processing units (GPUs), as well as the availability of large curated datasets, like ImageNet, for training models. This success has resulted in a substantial increase in interest and funding for AI research, with a notable rise in machine learning publications from 2015 to 2019.\n",
      "\n",
      "Furthermore, the rise of deep learning has also brought attention to important issues within AI, such as fairness and the ethical implications of technology, leading to a shift in focus for many researchers towards these critical areas.\n",
      "Sources:\n",
      "- https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "- https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "- https://en.wikipedia.org/wiki/Machine_learning\n",
      "- https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "\n",
      "Question: What are some applications of AI in healthcare?\n",
      "Answer: Some applications of AI in healthcare include:\n",
      "\n",
      "1. **Medical Diagnosis**: AI can assist in accurately diagnosing diseases by analyzing medical data and imaging.\n",
      "2. **Drug Discovery**: AI-guided drug discovery has been used to identify new classes of antibiotics and accelerate the search for treatments for diseases like Parkinson's.\n",
      "3. **Big Data Processing**: AI helps in processing and integrating large datasets, which is crucial for medical research and understanding complex biological pathways.\n",
      "4. **Microscopy Imaging**: AI is utilized in organoid and tissue engineering development, enhancing the analysis of microscopy images.\n",
      "5. **Predictive Analytics**: AI can predict patient outcomes and treatment responses, improving personalized medicine.\n",
      "6. **Real-Time Monitoring**: AI applications can monitor patient health in real-time, providing alerts for any critical changes.\n",
      "\n",
      "These applications aim to enhance patient care, improve treatment accuracy, and advance medical research.\n",
      "Sources:\n",
      "- https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "- https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "- https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "- https://en.wikipedia.org/wiki/Artificial_intelligence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "urls = [\"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "        \"https://en.wikipedia.org/wiki/Machine_learning\"]\n",
    "pdfs = []\n",
    "htmls = []\n",
    "\n",
    "rag_chain = setup_rag_chatbot(urls, pdfs, htmls, system_template, human_template, chunk_size, chunk_overlap, k, model_name)\n",
    "result = ask_rag_chatbot(\"What is artificial intelligence?\", rag_chain)\n",
    "print(f\"Question: {result['question']}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(\"Sources:\")\n",
    "for source in result['sources']:\n",
    "    print(f\"- {source}\")\n",
    "\n",
    "# What if multiple questions # \n",
    "questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does deep learning relate to AI?\",\n",
    "    \"What are some applications of AI in healthcare?\"\n",
    "]\n",
    "results = ask_multiple_questions(rag_chain, questions)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\nQuestion: {result['question']}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(\"Sources:\")\n",
    "    for source in result['sources']:\n",
    "        print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-GYbNF0m9Te"
   },
   "source": [
    "# Log to Deepchecks\n",
    "\n",
    "Can integrate Deepchecks LLM Eval to log and analyze performance of RAG chatbot\n",
    "Easy comparisons of different chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxDsQ-Kxpfpb"
   },
   "outputs": [],
   "source": [
    "!pip install -q deepchecks-llm-client  # Deepchecks Eval client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iB_xKlnmphPE"
   },
   "source": [
    "If looking to use Deepchecks client to log your RAG chatbot and appear in dashboard, else can avoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVpI-3R3pj8p"
   },
   "outputs": [],
   "source": [
    "from deepchecks_llm_client.client import DeepchecksLLMClient\n",
    "from deepchecks_llm_client.data_types import EnvType, AnnotationType, LogInteractionType, ApplicationType\n",
    "# Initialize Deepchecks LLM Eval client - need to login to GUI to get API key https://app.llm.deepchecks.com/configuration/api-key\n",
    "dc_client = DeepchecksLLMClient(\n",
    "    api_token=\"c3dpbGxpYW1zY290dDdAZ21haWwuY29t.b3JnX25vdl8yNF93b3Jrc2hvcF8xYjBkZTBlYzdhZGRlYTJh.aXyE4bhCwafQfgLHfSnm-Q\",  # Replace `\"YOUR_API_TOKEN_HERE\"` with your actual Deepchecks LLM Eval API token.\n",
    ")\n",
    "APP_NAME = f'RAGChatbot - \"Stuart\"' # appears in GUI dashboard like this\n",
    "dc_client.create_application(APP_NAME, ApplicationType.QA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EilPhDSpwYs"
   },
   "source": [
    "Now, let's modify our `ask_rag_chatbot` function to log interactions to Deepchecks LLM Eval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ttw6-omhpycD"
   },
   "outputs": [],
   "source": [
    "def ask_rag_chatbot(question, rag_chain, version_name):\n",
    "    result = rag_chain({\"query\": question})\n",
    "\n",
    "    # Log the interaction to Deepchecks LLM Eval\n",
    "    dc_client.log_interaction(\n",
    "        app_name=APP_NAME,\n",
    "        version_name=version_name,\n",
    "        env_type=EnvType.EVAL,\n",
    "        input=question,\n",
    "        output=result[\"result\"],\n",
    "        information_retrieval=[doc.page_content for doc in result[\"source_documents\"]],\n",
    "        user_interaction_id=hash(question)  # Each unique question will get it's own ID across all versions\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": result[\"result\"],\n",
    "        \"sources\": [doc.metadata.get('source', 'Unknown source') for doc in result[\"source_documents\"]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1mrHHsLLy12v"
   },
   "outputs": [],
   "source": [
    "def ask_multiple_questions(rag_chain, questions, version_name):\n",
    "    _ = dc_client.create_app_version(\n",
    "            app_name=APP_NAME,\n",
    "            version_name=version_name,\n",
    "            description=\"\"\n",
    "        )\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(partial(ask_rag_chatbot, rag_chain=rag_chain, version_name=version_name), questions), total=len(questions)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHPeFEp3rssh"
   },
   "source": [
    "Using deepcheck to compare different model configs for your chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9Prbd4Ym-1z",
    "outputId": "f3f92755-eb8e-4f29-b289-366bbd2882ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration 1 (chunk_size=1000, k=4):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration 2 (chunk_size=500, k=6):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does machine learning relate to AI?\",\n",
    "    \"What are some applications of deep learning?\"\n",
    "]\n",
    "\n",
    "# Can log interactions for different configurations\n",
    "print(\"Configuration 1 (chunk_size=1000, k=4):\")\n",
    "chunk_size_1, chunk_overlap_1, k_1 = 1000, 200, 4\n",
    "version_name = \"v1_chunk1000_k4\"\n",
    "rag_chain_1 = setup_rag_chatbot(urls, pdfs, htmls, system_template, human_template, chunk_size_1, chunk_overlap_1, k_1, model_name)\n",
    "_ = ask_multiple_questions(rag_chain_1, questions, version_name)\n",
    "\n",
    "\n",
    "print(\"Configuration 2 (chunk_size=500, k=6):\")\n",
    "chunk_size_2, chunk_overlap_2, k_2 = 500, 100, 6\n",
    "version_name = \"v2_chunk500_k6\"\n",
    "rag_chain_2 = setup_rag_chatbot(urls, pdfs, htmls, system_template, human_template, chunk_size_2, chunk_overlap_2, k_2, model_name)\n",
    "_ = ask_multiple_questions(rag_chain_2, questions, version_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdRdOYUmtSZn"
   },
   "source": [
    "1. Go to the Deepchecks LLM Eval web interface (https://app.llm.deepchecks.com).\n",
    "2. Select \"RAGChatbot\" application\n",
    "3. Compare different versions (e.g., \"v1_chunk1000_k4\" and \"v2_chunk500_k6\").\n",
    "4. Compare individual samples OR whole versions : https://app.llm.deepchecks.com/configuration/versions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "IlK08De8l6-2"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
